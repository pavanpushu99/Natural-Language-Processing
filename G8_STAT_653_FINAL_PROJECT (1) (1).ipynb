{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFvacJwaI8yP"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!pip install scrapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhDBnpOlJBq4"
      },
      "outputs": [],
      "source": [
        "!pip install fake_useragent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-8pvimhJBt3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scrapy.selector import Selector\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9If_2tzzJBwx",
        "outputId": "a82768bb-78b4-427e-ed1c-7862fad215a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page Title: Reviews: Avengers: Endgame - IMDb\n"
          ]
        }
      ],
      "source": [
        "#title\n",
        "from fake_useragent import UserAgent\n",
        "ua = UserAgent()\n",
        "userAgent = ua.chrome\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument(f'user-agent={userAgent}')\n",
        "# Create a new instance of the WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Navigate to a website\n",
        "url = 'https://www.imdb.com/title/tt4154796/reviews?spoiler=hide&sort=curated&dir=desc&ratingFilter=0'\n",
        "driver.get(url)\n",
        "\n",
        "# Get the page title\n",
        "page_title = driver.title\n",
        "print(f\"Page Title: {page_title}\")\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3x8UHHOJBzc"
      },
      "outputs": [],
      "source": [
        "#Reviews in page 1\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "# Set up Chrome WebDriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# IMDb URL\n",
        "url = 'https://www.imdb.com/title/tt4154796/reviews?spoiler=hide&sort=curated&dir=desc&ratingFilter=0'\n",
        "driver.get(url)\n",
        "\n",
        "# Wait for the reviews to load\n",
        "wait = WebDriverWait(driver, 10)\n",
        "wait.until(EC.visibility_of_element_located((By.CLASS_NAME, 'lister-list')))\n",
        "\n",
        "# Extract and print all review texts\n",
        "review_elements = driver.find_elements(By.CLASS_NAME, 'text.show-more__control')\n",
        "for i, review_element in enumerate(review_elements, start=1):\n",
        "    print(f\"Review {i}: {review_element.text.strip()}\")\n",
        "\n",
        "# Close the WebDriver\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDxBQ4V1bj02"
      },
      "outputs": [],
      "source": [
        "#all reviews and ratings\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "# Set up Chrome WebDriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# IMDb URL for Avengers: Endgame reviews\n",
        "url = 'https://www.imdb.com/title/tt4154796/reviews?spoiler=hide&sort=curated&dir=desc&ratingFilter=0'\n",
        "driver.get(url)\n",
        "\n",
        "# Wait for the reviews to load\n",
        "wait = WebDriverWait(driver, 10)\n",
        "wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'lister-list')))\n",
        "\n",
        "# Function to click \"Load More\" button until all reviews are loaded\n",
        "def load_all_reviews():\n",
        "    while True:\n",
        "        try:\n",
        "            load_more_button = driver.find_element(By.CLASS_NAME, 'ipl-load-more__button')\n",
        "            driver.execute_script(\"arguments[0].scrollIntoView();\", load_more_button)\n",
        "            load_more_button.click()\n",
        "            wait.until(EC.invisibility_of_element_located((By.CLASS_NAME, 'ipl-load-more__load-indicator')))\n",
        "        except Exception as e:\n",
        "            break\n",
        "\n",
        "# Function to extract ratings and reviews\n",
        "def extract_data():\n",
        "    rating_elements = driver.find_elements(By.CLASS_NAME, 'ipl-ratings-bar')\n",
        "    review_elements = driver.find_elements(By.CLASS_NAME, 'text')\n",
        "\n",
        "    data = []\n",
        "    for rating_element, review_element in zip(rating_elements, review_elements):\n",
        "        rating = rating_element.text.strip().split('\\n')[0]\n",
        "        review = review_element.text.strip()\n",
        "        data.append({'Rating': rating if rating else None, 'Review': review if review else None})\n",
        "        print(f\"Rating: {rating if rating else 'None'}\")\n",
        "        print(f\"Review: {review if review else 'None'}\\n\")\n",
        "    return data\n",
        "\n",
        "# Load all reviews and extract the ratings and reviews\n",
        "load_all_reviews()\n",
        "all_data = extract_data()\n",
        "\n",
        "# Close the WebDriver\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dORh5YIzZ94-"
      },
      "outputs": [],
      "source": [
        "# Extract text content from review elements\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame to store the ratings and reviews\n",
        "ratings_reviews_df = pd.DataFrame(all_data)\n",
        "\n",
        "# Save DataFrame to Excel with ratings and reviews in different columns\n",
        "output_file = \"/content/drive/MyDrive/Reviews.xlsx\"\n",
        "ratings_reviews_df.to_excel(output_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5jOKjmpi9SA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read the Excel file into a DataFrame\n",
        "ratings_reviews_df = pd.read_excel(\"/content/drive/MyDrive/Reviews.xlsx\")\n",
        "\n",
        "# Group the data by the \"ratings\" column and count occurrences\n",
        "ratings_counts = ratings_reviews_df['Rating'].value_counts()\n",
        "\n",
        "# Sort the ratings_counts Series in descending order\n",
        "ratings_counts = ratings_counts.sort_values(ascending=False)\n",
        "\n",
        "# Define colors for each rating value\n",
        "colors = ['blue', 'green', 'red', 'purple', 'orange', 'cyan', 'brown','pink','navy','yellow']\n",
        "\n",
        "# Plot a bar graph\n",
        "ax = ratings_counts.plot(kind='bar', color=colors)\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "\n",
        "# Add number counts on each bar\n",
        "for i, count in enumerate(ratings_counts):\n",
        "    ax.text(i, count + 10, str(count), ha='center', va='bottom')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU1HARCmn-qz"
      },
      "source": [
        "SENTIMENT ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxuqDikan-3i"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN65036Kn-58"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlzEc9xXn-9Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data from the Excel file\n",
        "input_file = \"/content/drive/MyDrive/Reviews.xlsx\"\n",
        "ratings_reviews_df = pd.read_excel(input_file)\n",
        "\n",
        "# Function to calculate sentiment score based on ratings\n",
        "def calculate_sentiment(rating):\n",
        "    # Extract the numerical part of the rating (e.g., \"7/10\" -> 7)\n",
        "    rating_value = float(rating.split('/')[0])\n",
        "    # Calculate sentiment score\n",
        "    sentiment_score = 1 if rating_value > 7 else 0\n",
        "    return sentiment_score\n",
        "\n",
        "# Apply the calculate_sentiment function to the 'Rating' column and add the result as a new column 'Sentiment'\n",
        "ratings_reviews_df['Sentiment'] = ratings_reviews_df['Rating'].apply(calculate_sentiment)\n",
        "\n",
        "# Save the updated DataFrame to Excel with the sentiment score added\n",
        "output_file = \"/content/drive/MyDrive/Reviews-1S.xlsx\"\n",
        "ratings_reviews_df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(ratings_reviews_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohxBN89XoG_s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "training = pd.read_excel(\"/content/drive/MyDrive/Reviews-1S.xlsx\", header=0)\n",
        "print(training.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Fstgu5FoJGE"
      },
      "outputs": [],
      "source": [
        "y_train = training['Sentiment']\n",
        "x_train = training.drop([\"Sentiment\",\"Rating\"], axis=1)\n",
        "\n",
        "print(x_train.head())\n",
        "print(y_train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsJvGjX0oMN4"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmlzVsruU9NF"
      },
      "source": [
        "CLEANING - DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqI3amtvVO5y"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#read file with reviews\n",
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file into a DataFrame\n",
        "\n",
        "df = pd.read_excel('/content/drive/MyDrive/Reviews-1S.xlsx')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4AxPN4LVdVr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "##### Training Data\n",
        "import pandas as pd\n",
        "training= pd.read_excel('/content/drive/MyDrive/Reviews-1S.xlsx')\n",
        "print(training.shape)\n",
        "training.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fkug5QfcVpmm"
      },
      "outputs": [],
      "source": [
        "# Data pre-processing\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import list of stopwords from library NLTK\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtUZmsIRVtl6"
      },
      "outputs": [],
      "source": [
        "# Remove Stopwords\n",
        "\n",
        "stopwords_list = stopwords.words(\"english\")\n",
        "print(f'List of stopwords:\\n{stopwords_list}\\n')\n",
        "\n",
        "# We remove negation words in list of stopwords\n",
        "no_stopwords = [\"not\",\"don't\",'aren','don','ain',\"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
        "               'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\",\n",
        "               \"won't\", 'wouldn', \"wouldn't\"]\n",
        "for no_stopword in no_stopwords:\n",
        "    stopwords_list.remove(no_stopword)\n",
        "\n",
        "print(f'Final list of stopwords:\\n{stopwords_list}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73x_5zM6VumK"
      },
      "outputs": [],
      "source": [
        "# Lemmatize reviews\n",
        "\n",
        "# Import Lemmatizer from NLTK\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# function that receive a list of words and do lemmatization:\n",
        "def lemma_stem_text(words_list):\n",
        "    # Lemmatizer\n",
        "    text = [lemmatizer.lemmatize(token.lower()) for token in words_list] # eighties->eight or messages->message or drugs->drug\n",
        "    text = [lemmatizer.lemmatize(token.lower(), \"v\") for token in text ]# going-> go or started->start or watching->watch\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp2uU0JNVxbz"
      },
      "outputs": [],
      "source": [
        "#Negative Contractions\n",
        "\n",
        "# create a function to change negative contractions (e.g. isn't, can't, ... etc.) to standard from using a regular expression.\n",
        "import re\n",
        "re_negation = re.compile(\"n't \") # specify a pattern you want to find in a string\n",
        "\n",
        "# function that receive a sequence of words and return the same sequence transforming\n",
        "# abbreviated negations to the standard form.\n",
        "def negation_abbreviated_to_standard(sent):\n",
        "    sent = re_negation.sub(\" not \", sent)\n",
        "    return sent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRq0FqyuV0PZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1142193-e0e4-450e-bea5-0e3a73a8a754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "#REVIEW\n",
        "\n",
        "# create a function to clean the text of a review using the functions defined previously.\n",
        "\n",
        "# Import function BeautifulSoup to clean text of HTML tags\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "\n",
        "def review_to_words(raw_review):\n",
        "    # 1. Remove HTML tags\n",
        "    review_text = BeautifulSoup(raw_review).get_text()\n",
        "\n",
        "    # 2. Transform abbreviated negations to the standard form.\n",
        "    review_text = negation_abbreviated_to_standard(review_text)\n",
        "\n",
        "    # 3. Remove non-letters and non-numbers\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    words = tokenizer.tokenize(review_text)\n",
        "\n",
        "    # 4. Remove stop words\n",
        "    meaningful_words = [w for w in words if w.lower() not in stopwords_list]\n",
        "\n",
        "    # 5. Apply lemmatization function\n",
        "    lemma_words = lemma_stem_text(meaningful_words)\n",
        "\n",
        "    # 6. Join the words back into one string separated by space, and return the result.\n",
        "    return( \" \".join(lemma_words))\n",
        "\n",
        "    # 7. Handling Short Words\n",
        "    tokens = [word for word in tokens if len(word) >= 3]\n",
        "\n",
        "    # 8. Handling Rare Words\n",
        "    word_counts = Counter(tokens)\n",
        "    common_words = set(word for word, count in word_counts.items() if count > 5)\n",
        "    tokens = [word for word in tokens if word in common_words]\n",
        "\n",
        "    # 9. # Drop rows with NaN values in the 'Reviews' column\n",
        "    training.dropna(subset=['Review'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKdD-8H0V3Lu"
      },
      "outputs": [],
      "source": [
        "# Clean the first review\n",
        "clean_review = review_to_words(training[\"Review\"].iloc[0])\n",
        "\n",
        "# Print original review, and cleaned review\n",
        "print(f'Text of original review:\\n{training[\"Review\"].iloc[0]}\\n')\n",
        "print(f'Text of cleaned review:\\n{clean_review}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n9SDQ4CV5yV"
      },
      "outputs": [],
      "source": [
        "# We get the text of reviews in the training set\n",
        "reviews = x_train['Review']\n",
        "\n",
        "# We initialize an empty list to add the clean reviews\n",
        "cleaned_train_reviews = []\n",
        "\n",
        "# We loop over each review and clean it\n",
        "for i in reviews:\n",
        "    cleaned_train_reviews.append(review_to_words(i))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDBx_-AeV8WV"
      },
      "outputs": [],
      "source": [
        "# Print ALL cleaned reviews\n",
        "print(cleaned_train_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5N_du-brdJo"
      },
      "outputs": [],
      "source": [
        "#wordcloud\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Join all cleaned reviews into a single string\n",
        "all_reviews_text = ' '.join(cleaned_train_reviews)\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_reviews_text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZUGldJpWj3D"
      },
      "source": [
        "DOCUMENT TERM MATRIX- (DTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rinI0U43o2K4"
      },
      "source": [
        "logistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4XV9cKBWUmd"
      },
      "outputs": [],
      "source": [
        "# Import Count Vectorizer encoding from sklearn library\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "c = CountVectorizer(stop_words='english', lowercase=True, token_pattern=r'\\w+', ngram_range=(1, 2))\n",
        "\n",
        "# Fit and transform y_train to generate the Bigram DTM\n",
        "train_data_features_d = c.fit_transform(cleaned_train_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDweE_UVWUoo"
      },
      "outputs": [],
      "source": [
        "# We split train dataset to create validation set and training the Logistic Regression Model on validation set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_d, X_val_d, Y_train_d, Y_val_d = train_test_split(train_data_features_d, y_train, test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRsMDjWNWUsE"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize a logistic regression model\n",
        "logistic_d = LogisticRegression(random_state=0)\n",
        "# Train the model\n",
        "logistic_d.fit(X_train_d, Y_train_d)\n",
        "\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred = logistic_d.predict(X_val_d)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o13nCvoWy7T"
      },
      "outputs": [],
      "source": [
        "#Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "Y_pred_logistic_d  = logistic_d.predict(X_val_d)\n",
        "confusion_matrix_logistic_d = confusion_matrix(Y_val_d, Y_pred_logistic_d, labels=[1,0])\n",
        "confusion_matrix_logistic_d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2--ICmsW1AP"
      },
      "outputs": [],
      "source": [
        "# plot the confusion matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ax = plt.axes()\n",
        "sns.heatmap(confusion_matrix_logistic_d, annot=True, fmt=\"d\")\n",
        "ax.set_title('Confusion matrix Logistic Regression')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Lz6zybJHyPm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score,roc_auc_score, roc_curve\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Calculate accuracy and round to 2 decimal places\n",
        "accuracy = round(accuracy_score(Y_val_d, y_pred), 2)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred_d = logistic_d.predict(X_val_d)\n",
        "\n",
        "# Calculate precision\n",
        "precision_d = precision_score(Y_val_d, y_pred_d)\n",
        "\n",
        "# Calculate recall\n",
        "recall_d = recall_score(Y_val_d, y_pred_d)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1_d = f1_score(Y_val_d, y_pred_d)\n",
        "\n",
        "\n",
        "print(\"Precision (DTM):\", precision_d)\n",
        "print(\"Recall (DTM):\", recall_d)\n",
        "print(\"F1 Score (DTM):\", f1_d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK_1JxO7pKLk"
      },
      "source": [
        "**svm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KSbBxs5p7Bu"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Initialize SVM model\n",
        "svm_model = SVC(random_state=0)\n",
        "\n",
        "#Train the SVM model\n",
        "svm_model.fit(X_train_d, Y_train_d)\n",
        "\n",
        "#Predict on the validation set\n",
        "Y_pred_svm_d = svm_model.predict(X_val_d)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijONhZAtqOPR"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy for SVM model\n",
        "accuracy_svm = accuracy_score(Y_val_d, Y_pred_svm_d)\n",
        "precision_svm = precision_score(Y_val_d, Y_pred_svm_d)\n",
        "recall_svm = recall_score(Y_val_d, Y_pred_svm_d)\n",
        "f1_svm = f1_score(Y_val_d, Y_pred_svm_d)\n",
        "\n",
        "print(\"\\nSVM Model Metrics:\")\n",
        "print(\"Accuracy:\", accuracy_svm)\n",
        "print(\"Precision:\", precision_svm)\n",
        "print(\"Recall:\", recall_svm)\n",
        "print(\"F1 Score:\", f1_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfyfgcAfshnw"
      },
      "source": [
        "random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUcN7MaVsk7w"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Initialize Random Forest model\n",
        "random_forest_model = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Train the Random Forest model\n",
        "random_forest_model.fit(X_train_d, Y_train_d)\n",
        "\n",
        "# Predict on the validation set\n",
        "Y_pred_rf_d = random_forest_model.predict(X_val_d)\n",
        "\n",
        "# Calculate metrics for Random Forest model\n",
        "accuracy_rf = accuracy_score(Y_val_d, Y_pred_rf_d)\n",
        "precision_rf = precision_score(Y_val_d, Y_pred_rf_d)\n",
        "recall_rf = recall_score(Y_val_d, Y_pred_rf_d)\n",
        "f1_rf = f1_score(Y_val_d, Y_pred_rf_d)\n",
        "\n",
        "print(\"Random Forest Model Metrics:\")\n",
        "print(\"Accuracy:\", accuracy_rf)\n",
        "print(\"Precision:\", precision_rf)\n",
        "print(\"Recall:\", recall_rf)\n",
        "print(\"F1 Score:\", f1_rf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKO_tBAVW3ni"
      },
      "source": [
        "TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqmZy71xW5Ma"
      },
      "outputs": [],
      "source": [
        "# Import tf-idf encoding from sklearn library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define some hyperparameters of encoded\n",
        "vectorizer = TfidfVectorizer(min_df=0.5, max_df=0.90,ngram_range = (1,2))\n",
        "\n",
        "# Create the training set with the words encoded as features of the reviews\n",
        "train_data_features_t = vectorizer.fit_transform(cleaned_train_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmPVLQT6W5Ph"
      },
      "outputs": [],
      "source": [
        "# We split train dataset to create validation set and training the Logistic Regression Model on validation set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_t, X_val_t, Y_train_t, Y_val_t = train_test_split(train_data_features_t, y_train, test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMEdhdTlW5S6"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize a logistic regression model\n",
        "logistic_t = LogisticRegression(random_state=0)\n",
        "# Train the model\n",
        "logistic_t = logistic_t.fit(X_train_t, Y_train_t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC37x8tDXA7j"
      },
      "outputs": [],
      "source": [
        "#Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "Y_pred_logistic_t  = logistic_t.predict(X_val_t)\n",
        "confusion_matrix_logistic_t = confusion_matrix(Y_val_t, Y_pred_logistic_t, labels=[1,0])\n",
        "confusion_matrix_logistic_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do-aRHQRXBi2"
      },
      "outputs": [],
      "source": [
        "# plot the confusion matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ax = plt.axes()\n",
        "sns.heatmap(confusion_matrix_logistic_t, annot=True, fmt=\"d\")\n",
        "ax.set_title('Confusion matrix Logistic Regression')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5J4X9QBIj5c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred_t = logistic_t.predict(X_val_t)\n",
        "\n",
        "# Calculate metrics for logistic regression model\n",
        "accuracy_logistic_t = accuracy_score(Y_val_t, y_pred_t)\n",
        "precision_logistic_t = precision_score(Y_val_t, y_pred_t)\n",
        "recall_logistic_t = recall_score(Y_val_t, y_pred_t)\n",
        "f1_logistic_t = f1_score(Y_val_t, y_pred_t)\n",
        "\n",
        "print(\"Logistic Regression Model Metrics on TF-IDF encoded data:\")\n",
        "print(\"Accuracy:\", accuracy_logistic_t)\n",
        "print(\"Precision:\", precision_logistic_t)\n",
        "print(\"Recall:\", recall_logistic_t)\n",
        "print(\"F1 Score:\", f1_logistic_t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oYNI2oJo9gv"
      },
      "source": [
        "svm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA0vxS3Uo82G"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVM model\n",
        "svm_model_tfidf = SVC(random_state=0)\n",
        "\n",
        "# Train the SVM model\n",
        "svm_model_tfidf.fit(X_train_t, Y_train_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDx1RckYqiTQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred_svm_tfidf = svm_model_tfidf.predict(X_val_t)\n",
        "\n",
        "# Calculate metrics for SVM model\n",
        "accuracy_svm_tfidf = accuracy_score(Y_val_t, y_pred_svm_tfidf)\n",
        "precision_svm_tfidf = precision_score(Y_val_t, y_pred_svm_tfidf)\n",
        "recall_svm_tfidf = recall_score(Y_val_t, y_pred_svm_tfidf)\n",
        "f1_svm_tfidf = f1_score(Y_val_t, y_pred_svm_tfidf)\n",
        "\n",
        "print(\"SVM Model Metrics on TF-IDF encoded data:\")\n",
        "print(\"Accuracy:\", accuracy_svm_tfidf)\n",
        "print(\"Precision:\", precision_svm_tfidf)\n",
        "print(\"Recall:\", recall_svm_tfidf)\n",
        "print(\"F1 Score:\", f1_svm_tfidf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWgO4dFprUf3"
      },
      "source": [
        "random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AnENyxfrSbn"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf_model.fit(X_train_t, Y_train_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxdGN9U_rrPL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred_rf_t = rf_model.predict(X_val_t)\n",
        "\n",
        "# Calculate metrics for Random Forest model\n",
        "accuracy_rf_t = accuracy_score(Y_val_t, y_pred_rf_t)\n",
        "precision_rf_t = precision_score(Y_val_t, y_pred_rf_t)\n",
        "recall_rf_t = recall_score(Y_val_t, y_pred_rf_t)\n",
        "f1_rf_t = f1_score(Y_val_t, y_pred_rf_t)\n",
        "\n",
        "print(\"Random Forest Model Metrics:\")\n",
        "print(\"Accuracy:\", accuracy_rf_t)\n",
        "print(\"Precision:\", precision_rf_t)\n",
        "print(\"Recall:\", recall_rf_t)\n",
        "print(\"F1 Score:\", f1_rf_t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDQZd-crXGHA"
      },
      "source": [
        "WORD EMBEDDINGS - (word2vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnDZGyLw4JhF"
      },
      "outputs": [],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJVwhNs8asDD"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "\n",
        "# Tokenize the cleaned reviews\n",
        "tokenized_reviews = [review.split() for review in cleaned_train_reviews]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = gensim.models.Word2Vec(\n",
        "    tokenized_reviews,\n",
        "    vector_size=150,\n",
        "    window=10,\n",
        "    min_count=3,\n",
        "    workers=10\n",
        ")\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "model.train(tokenized_reviews, total_examples=len(tokenized_reviews), epochs=10)\n",
        "\n",
        "# Save the trained word vectors\n",
        "word_vectors = model.wv\n",
        "word_vectors.save(\"/content/drive/MyDrive/STAT653/my_w2v.kv\")\n",
        "\n",
        "# Load the saved word vectors\n",
        "loaded_word_vectors = gensim.models.KeyedVectors.load(\"/content/drive/MyDrive/STAT653/my_w2v.kv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WYIpKwzbJKg"
      },
      "outputs": [],
      "source": [
        "w1 = \"thanos\"\n",
        "loaded_word_vectors.most_similar (positive=w1,topn=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcHAbxPubfm4"
      },
      "outputs": [],
      "source": [
        "w2 = \"love\"\n",
        "loaded_word_vectors.most_similar (positive=w2, topn=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zeq7EIJhcKfU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtJ1vZ4O4RUc"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenize the cleaned reviews\n",
        "tokenized_reviews = [review.split() for review in cleaned_train_reviews]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the Word2Vec model\n",
        "word2vec_model.save(\"/content/drive/MyDrive/word2vec_model.model\")\n",
        "\n",
        "word2vec_model = Word2Vec.load(\"/content/drive/MyDrive/word2vec_model.model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VXzAKJU4ZDj"
      },
      "outputs": [],
      "source": [
        "similar_words = word2vec_model.wv.most_similar('great', topn=5)\n",
        "print(similar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWQXpDrI6CsP"
      },
      "outputs": [],
      "source": [
        "similar_words = word2vec_model.wv.most_similar('thanos', topn=5)\n",
        "print(similar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OcfTTyo65KQ"
      },
      "outputs": [],
      "source": [
        "w1 = \"love\"\n",
        "similar_words = word2vec_model.wv.most_similar(positive=w1, topn=3)\n",
        "print(similar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CXfvMSe7S1h"
      },
      "outputs": [],
      "source": [
        "similarity_score = word2vec_model.wv.similarity(w1=\"love\", w2=\"3000\")\n",
        "\n",
        "# Print the similarity score\n",
        "print(similarity_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh3cKnwg7iNU"
      },
      "outputs": [],
      "source": [
        "# Find the odd word out\n",
        "odd_one_out = word2vec_model.wv.doesnt_match([\"thor\", \"thanos\", \"captain\"])\n",
        "\n",
        "# Print the odd word out\n",
        "print(odd_one_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajMXECb179VZ"
      },
      "outputs": [],
      "source": [
        "# Find the odd word out\n",
        "odd_one_out = word2vec_model.wv.doesnt_match([\"film\", \"thor\", \"ironman\"])\n",
        "\n",
        "# Print the odd word out\n",
        "print(odd_one_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nYpbnLFSL_p"
      },
      "source": [
        "FASTTEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzvKJFNokoAs"
      },
      "outputs": [],
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.models import FastText\n",
        "import pandas as pd\n",
        "\n",
        "tokenized_reviews = [review.split() for review in cleaned_train_reviews]\n",
        "phrases = Phrases(tokenized_reviews, min_count = 30, progress_per = 10000)\n",
        "sentences = phrases[tokenized_reviews]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi2-w1Crk8hW"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = FastText(vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Building Vocabulary\n",
        "model.build_vocab(sentences)\n",
        "\n",
        "# Get the keys using the .index_to_key method\n",
        "keys = model.wv.index_to_key\n",
        "\n",
        "print(len(keys))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgdQy7anlGhj"
      },
      "outputs": [],
      "source": [
        "#Training the model\n",
        "model.train(sentences, total_examples=model.corpus_count, epochs=10)\n",
        "\n",
        "# Save the trained model using joblib\n",
        "import joblib\n",
        "joblib.dump(model, '/content/drive/MyDrive/STAT653/fasttext_model.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KReHSwK6lLlj"
      },
      "outputs": [],
      "source": [
        "vocabulary = model.wv.index_to_key\n",
        "print('uniform' in vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8-oNQaZoU-T"
      },
      "outputs": [],
      "source": [
        "vocabulary = model.wv.index_to_key\n",
        "print('3000' in vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igbOKbNVl0l5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cDwmTREl0t6"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "# Tokenize your reviews data\n",
        "tokenized_reviews = [review.split() for review in cleaned_train_reviews]\n",
        "\n",
        "# Train the FastText model\n",
        "model = FastText(sentences=tokenized_reviews, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/MyDrive/fasttext_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD_1WGkzSPxw"
      },
      "outputs": [],
      "source": [
        "# Calculate similarity between two words\n",
        "similarity = model.wv.similarity(\"great\", \"amazing\")\n",
        "print(\"Similarity between 'great' and 'amazing':\", similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7aDsaVaSRoc"
      },
      "outputs": [],
      "source": [
        "# Calculate similarity between two words\n",
        "similarity = model.wv.similarity(\"action\", \"adventure\")\n",
        "print(\"Similarity between 'action' and 'adventure':\", similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHgtFeOqSVnR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StOI3CKwSVpk"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext\n",
        "import fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icOQ2H4eSVsA"
      },
      "outputs": [],
      "source": [
        "# Define the path for the output file\n",
        "output_file = '/content/drive/MyDrive/cleaned_train_reviews.txt'\n",
        "\n",
        "# Write the cleaned reviews to the output file\n",
        "with open(output_file, 'w') as file:\n",
        "    file.write('\\n'.join(cleaned_train_reviews))\n",
        "\n",
        "print(\"Cleaned train reviews have been saved in:\", output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THKWYNyASVth"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "import time\n",
        "start_time = time.time()\n",
        "path = '/content/drive/MyDrive/cleaned_train_reviews.txt'\n",
        "model = fasttext.train_unsupervised(path , model='skipgram')\n",
        "end_time = time.time()\n",
        "print(f\"training time took {round(end_time - start_time, 3)} secs\")\n",
        "print(model.words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIHtuxjoSVw3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "print(cosine_similarity([model['performance']], [model['beautiful']]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}